{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup as bs\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "import pandas as pd\n",
    "import regex as re\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AmazonScrapper:\n",
    "    \"\"\"\n",
    "    This Program or Whatever you call it is a webscrapper for the Amazon website\n",
    "    It searches and stores the products that fit in the following criteria:\n",
    "    - Has only One Seller\n",
    "    - Not in the blacklisted Brandlist given\n",
    "    - bsr number within the range of 1 to 150,000\n",
    "\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, base_url, paging_url,\n",
    "                 driver_path='C:/Users/Jimoh/Documents/ML projects/Scraping/chrome/chromedriver_win32/chromedriver.exe', \n",
    "                 brand_list_path = 'C:/Users/Jimoh/Documents/GitHub/Scraper/docs/brandlist/brandList.csv'):\n",
    "        \n",
    "        \"\"\"\n",
    "        Here the paging url, first page url and number of pages the product search has are given\n",
    "        The paths to the chrome driver and blacklisted brandlist is given\n",
    "        \n",
    "        \"\"\"\n",
    "        self.driver_path = driver_path\n",
    "        self.base_url = base_url\n",
    "#         self.no_of_pages = no_of_pages\n",
    "        self.paging_url = paging_url\n",
    "        self.proc_urls()\n",
    "        self.set_brand_list(brand_list_path)\n",
    "        \n",
    "    def proc_urls(self):\n",
    "        \"\"\"\n",
    "        The paging url is formatted so it can be reproducable for multiple pages\n",
    "        \n",
    "        \"\"\"\n",
    "        try:\n",
    "            self.base_url = self.base_url.replace(re.findall('qid=\\d+&*', self.paging_url)[0], '')\n",
    "        except IndexError:\n",
    "            pass\n",
    "        try:\n",
    "            self.paging_url = self.paging_url.replace(re.findall('qid=\\d+&*', self.paging_url)[0], '')\n",
    "        except IndexError:\n",
    "            pass\n",
    "        self.paging_url = self.paging_url.replace(re.findall('page=\\d', self.paging_url)[0], 'page={}')\n",
    "        self.paging_url = self.paging_url.replace(re.findall('sr_pg_\\d', self.paging_url)[0], 'sr_pg_{}')\n",
    "        \n",
    "    def set_brand_list(self, path):\n",
    "        \"\"\"\n",
    "        Makes the Black listed brand lists\n",
    "        \n",
    "        \"\"\"\n",
    "        self.brand_list = pd.read_csv(path).set_index('0')#.iloc[:,0].to_list()\n",
    "    \n",
    "    def set_src(self, page_no=1, err = 1):\n",
    "        \"\"\" \n",
    "        Sets the HTML source code of the page in class variabe src\n",
    "        \n",
    "        \"\"\"\n",
    "        try:\n",
    "            options = Options()\n",
    "            # connects the driver to the already opened browser with remote debugging\n",
    "            options.add_experimental_option('debuggerAddress', 'localhost:9222')\n",
    "\n",
    "            driver = webdriver.Chrome(options=options, executable_path=self.driver_path)\n",
    "            if page_no == 1:\n",
    "                driver.get(self.base_url)\n",
    "            elif page_no > 1:\n",
    "                driver.get(self.get_paging_url(page_no))\n",
    "\n",
    "            self.src = bs(driver.page_source, features= 'html.parser')\n",
    "            self.set_product_panes()\n",
    "            driver.quit()\n",
    "        except:\n",
    "            if err <= 3:\n",
    "                time.sleep(2)\n",
    "                self.set_src(page_no, err = err + 1)\n",
    "                if err == 3:\n",
    "                    print('couldn\\'t get src')\n",
    "            return ''\n",
    "            \n",
    "    \n",
    "    def set_product_panes(self):\n",
    "        \"\"\"\n",
    "        Returns the product holders' source codes\n",
    "        \n",
    "        \"\"\"\n",
    "        self.product_panes = self.src.find_all('div', {'data-component-type':'s-search-result'})\n",
    "        \n",
    "        \n",
    "         \n",
    "    def get_paging_url(self, page_no):\n",
    "        \"\"\"\n",
    "        Gets the paging url from the user\n",
    "\n",
    "        \"\"\"\n",
    "        return self.paging_url.format(page_no, page_no)\n",
    "\n",
    "    def get_product_asin(self, product_pane):\n",
    "        \"\"\"\n",
    "        Gets the product ASIN\n",
    "        \n",
    "        \"\"\"\n",
    "        return product_pane.attrs['data-asin']\n",
    "            \n",
    "        \n",
    "    def get_product_desc(self, product_pane):\n",
    "        \"\"\"\n",
    "        Gets the product description\n",
    "        \n",
    "        \"\"\"    \n",
    "        product_desc = product_pane.h2.a.text.strip()\n",
    "        return product_desc\n",
    "    \n",
    "    def get_product_brand(self, product_pane):\n",
    "        options = self.get_product_desc(product_pane).split()[:2]\n",
    "        for i in options:\n",
    "            try:\n",
    "                int(i)\n",
    "            except ValueError:\n",
    "                brand = i.upper()\n",
    "                break\n",
    "        return brand\n",
    "        \n",
    "    def get_product_link(self, product_pane):\n",
    "        \"\"\"\n",
    "        Gets the url link to the product\n",
    "        \n",
    "        \"\"\"\n",
    "        product_link = 'https://www.amazon.com' + product_pane.h2.a['href']\n",
    "        return product_link\n",
    "    \n",
    "    def get_keepa_link(self, product_pane):\n",
    "        f = 'https://keepa.com/#!product/1-{}'\n",
    "        asin = self.get_product_asin(product_pane)\n",
    "        \n",
    "        keepa_link = f.format(asin)\n",
    "        return keepa_link\n",
    "    \n",
    "    def get_ccc_links(self, product_pane):\n",
    "        f = 'https://charts.camelcamelcamel.com/us/{}/amazon.png?force=1&zero=0&w=594&h=356&desired=false&legend=1&ilt=1&tp={}&fo=1&lang=en'\n",
    "        f1 = 'https://charts.camelcamelcamel.com/us/{}/amazon-new.png?force=1&zero=0&w=594&h=356&desired=false&legend=1&ilt=1&tp={}&fo=1&lang=en'\n",
    "        asin = self.get_product_asin(product_pane)\n",
    "        \n",
    "        m_ccc = f.format(asin, '3m')\n",
    "        all_ccc = f1.format(asin, 'all')\n",
    "        \n",
    "        return all_ccc, m_ccc\n",
    "        \n",
    "    def get_product_price(self, product_pane):\n",
    "        \"\"\"\n",
    "        Gets the product price\n",
    "        \n",
    "        \"\"\"\n",
    "        try:\n",
    "            product_price_pane = product_pane.find('span', 'a-price')\n",
    "            product_price = float(product_price_pane.find('span', 'a-offscreen').text.strip().replace(',','')[1:])\n",
    "            return product_price\n",
    "        except AttributeError:\n",
    "            return -1\n",
    "        \n",
    "    def get_helium_pane(self, product_pane):\n",
    "        \"\"\"\n",
    "        Gets the src codes of the pane rendered by the helium 10 extension\n",
    "        \n",
    "        \"\"\"\n",
    "        helium_pane = product_pane.find('div', {'id':re.compile('bsr-\\w+')}).div\n",
    "        return helium_pane\n",
    "\n",
    "    def filter_asin_by_disallowed_brands(self, product_pane):\n",
    "        \"\"\"\n",
    "        filters the product by checking if it is not in the blacklisted brand list\n",
    "        if it passes return 1, otherwise 0\n",
    "        \n",
    "        \"\"\"\n",
    "        brand = self.get_product_brand(product_pane)\n",
    "        sub_brand_list = list(self.brand_list.filter(like= brand[:3], axis= 'index').index)\n",
    "\n",
    "        for i in sub_brand_list:\n",
    "            if brand[:-1] in i:\n",
    "                return 0\n",
    "        return 1\n",
    "    \n",
    "    def filter_asin_by_price(self, product_pane):\n",
    "        \"\"\"\n",
    "        Filters the product the price\n",
    "        \n",
    "        \"\"\"\n",
    "        price = self.get_product_price(product_pane)\n",
    "        if (price == -1):\n",
    "            return 0\n",
    "        else:\n",
    "            return 1\n",
    "\n",
    "    def filter_asin_by_seller_count(self, product_pane, count = 1):\n",
    "        \"\"\"\n",
    "        filters the product by making sure the number of sellers is less than the threshold given\n",
    "        if it passes return 1, otherwise 0 (helium extension used)\n",
    "        \n",
    "        \"\"\"\n",
    "        helium_pane = self.get_helium_pane(product_pane)\n",
    "        try:\n",
    "            seller_count = helium_pane.findAll('div', recursive= False)[1].findAll('div', recursive= False)[1].findAll('a')[1].text.split(' ')[0].replace(',', '')\n",
    "        except IndexError:\n",
    "            print('err0: ')\n",
    "            return 0\n",
    "\n",
    "        if int(seller_count) > count:\n",
    "            return 0\n",
    "        else:\n",
    "            return 1\n",
    "\n",
    "\n",
    "    def filter_asin_by_bsr_no(self, product_pane):\n",
    "        \"\"\"\n",
    "        filters the product by making sure it falls into the allowed range of bsr ranking (1 - 150,000)\n",
    "        if it passes return 1, otherwise 0 (helium extension used)\n",
    "        \"\"\"\n",
    "        helium_pane = self.get_helium_pane(product_pane)\n",
    "        for i in range(len(helium_pane.findAll('div', recursive= False)[0].findAll('div', recursive = False))):\n",
    "            bsr_data = helium_pane.findAll('div', recursive= False)[0].findAll('div', recursive = False)[i].text\n",
    "            try:\n",
    "                bsr_data = bsr_data.split()[0][1:].replace(',','')\n",
    "            except IndexError:\n",
    "                print('err1: ' + bsr_data)\n",
    "                return 0\n",
    "            try:\n",
    "                if int(bsr_data) > 150000:\n",
    "                    return 0      \n",
    "            except ValueError:\n",
    "                print('err2: ' + bsr_data)\n",
    "                return 0\n",
    "        return 1\n",
    "    \n",
    "    def apply_filters_on_pane(self, product_pane):\n",
    "        \"\"\"\n",
    "        Applies all the given filters on the product\n",
    "        \n",
    "        \"\"\"\n",
    "        result = []\n",
    "        result.append(self.filter_asin_by_seller_count(product_pane))\n",
    "        result.append(self.filter_asin_by_disallowed_brands(product_pane))\n",
    "        result.append(self.filter_asin_by_bsr_no(product_pane))\n",
    "        result.append(self.filter_asin_by_price(product_pane))\n",
    "        return result\n",
    "    \n",
    "    def apply_filters_on_page(self, product_panes):\n",
    "        \"\"\"\n",
    "        Applies the filters on each product on the page and returns the qualified ones\n",
    "        \n",
    "        \"\"\"\n",
    "        valid_products = []\n",
    "        for i in product_panes:\n",
    "            filters = self.apply_filters_on_pane(i)\n",
    "            if 0 not in filters:\n",
    "                valid_products.append((self.get_product_asin(i), \n",
    "                                      self.get_product_desc(i),\n",
    "                                      self.get_product_brand(i),\n",
    "                                      self.get_product_link(i), \n",
    "                                      self.get_product_price(i),\n",
    "#                                       self.get_ccc_links(i)[0],\n",
    "#                                       self.get_ccc_links(i)[1],\n",
    "                                      self.get_keepa_link(i)\n",
    "                                      ))\n",
    "        return valid_products\n",
    "    \n",
    "    \n",
    "    def scrape_pages(self,  npages, no_of_products= 100):\n",
    "        \"\"\"\n",
    "        Getting the qualified products after applying filters for multiple pages\n",
    "        \n",
    "        \"\"\"\n",
    "        try:\n",
    "            products = []\n",
    "            if type(npages) == int:\n",
    "                arr = range(1, npages+1, 1)\n",
    "            elif type(npages) == list or type(npages) == tuple:\n",
    "                if len(npages) == 2:\n",
    "                    arr = range(npages[0], npages[1])\n",
    "                elif len(npages) == 3:\n",
    "                    arr = range(npages[0], npages[1], npages[2])\n",
    "            for i in arr:\n",
    "                if i == 1:\n",
    "                    src = self.set_src()\n",
    "                    if src == '':\n",
    "                        continue            \n",
    "                elif i > 1:\n",
    "                    src = self.set_src(i)\n",
    "                    if src  == '':\n",
    "                        continue\n",
    "                        \n",
    "                panes = self.product_panes\n",
    "                product = self.apply_filters_on_page(panes)\n",
    "                products += product\n",
    "                print(f'Page {i:4} ==> {len(products):4} product(s) found')\n",
    "\n",
    "                if len(products) >= no_of_products:\n",
    "                    break\n",
    "                if len(products) in range(10, 401, 10):\n",
    "                    self.products_df = pd.DataFrame(set(products), columns=['ASIN', 'Description', 'Brand', 'Link', 'Price', 'Total Duration'])\n",
    "                    self.products_df['Price+10%'] = pd.NA\n",
    "                    self.products_df['Defects'] = pd.NA\n",
    "                    self.save_result()\n",
    "\n",
    "            self.products_df = pd.DataFrame(set(products), columns=['ASIN', 'Description', 'Brand', 'Link', 'Price', 'Total Duration'])\n",
    "            self.products_df['Price+10%'] = pd.NA\n",
    "            self.products_df['Defects'] = pd.NA\n",
    "            self.save_result()\n",
    "            return self.products_df, i\n",
    "        \n",
    "        except KeyboardInterrupt:\n",
    "            self.products_df = pd.DataFrame(set(products), columns=['ASIN', 'Description', 'Brand', 'Link', 'Price', 'Total Duration'])\n",
    "            self.products_df['Price+10%'] = pd.NA\n",
    "            self.products_df['Defects'] = pd.NA\n",
    "            self.save_result()\n",
    "            return self.products_df, i\n",
    "    \n",
    "     \n",
    "    def save_result(self, loc = 'products_extra_to_seller_testth.xlsx'):\n",
    "        self.products_df.to_excel('C:/Users/Jimoh/Documents/GitHub/Scraper/docs/' + loc, index= False)\n",
    "        return 'success'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "base = 'https://www.amazon.com/s?k=oven&i=garden&rh=p_6%3AATVPDKIKX0DER%2Cp_36%3A2000-&dc&qid=1627284445&rnid=386465011&ref=sr_nr_p_36_5'\n",
    "\n",
    "page = 'https://www.amazon.com/s?k=oven&i=garden&rh=p_6%3AATVPDKIKX0DER%2Cp_36%3A2000-&dc&page=2&qid=1627284486&rnid=386465011&ref=sr_pg_2'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "scraper = AmazonScrapper(base, page)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Page    5 ==>    2 product(s) found\n",
      "Page    6 ==>    2 product(s) found\n",
      "err0: \n",
      "err1: \n",
      "Page    7 ==>    2 product(s) found\n",
      "Page    8 ==>    2 product(s) found\n",
      "Page    9 ==>    2 product(s) found\n",
      "Wall time: 1min 55s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "df, no_of_pages_scrapped = scraper.scrape_pages([5,10], 200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# page 105, keyword -thfjfd keyboards\n",
    "# Wall time: 1h 52min 59s\n",
    "\n",
    "# page 60 keyword oven"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'success'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scraper.save_result()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "scraper.products_df.to_excel('C:/Users/Jimoh/Documents/GitHub/Scraper/docs/products_extra_to_seller_12th.xlsx', index= False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
